{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correos electronicos SPAM: Un enfoque con Procesamiento de Lenguaje Natural\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uriel Antonio Alvarez Chavez 16111783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los correos electronicos no deseados en su bandeja de entrada son molestos ya que perturban la rutina del usuario.Es por eso que las cuentas de correo electronico ya tiene un filtro de spam. Dadoo que es una de las aplicaciones PLN mas utilizadas vamos a ver como se derarrolla un filtro spam simple para correos electronicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserta los datos\n",
    "full_corpus = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header=None, names =['label','msg_body'] )\n",
    "\n",
    "#Separando los mensajes en 'ham'y 'spam'\n",
    "ham_text=[]\n",
    "spam_text=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspeccio y separacion de mensajes en las categorias \"ham\" y \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente el conjunto de datos debe inspeccionarce para ocuparlo y abordarlo para lograr la tarea. El formato de los datos ddos, la cantidad de datos proporcionados, la naturaleza de los datos se incluyen en esta inspeccion para identificar la mejor aproximacion posible para la tarea.\n",
    "\n",
    "el corpus de menses dado ha marcado cada mensjae como ham o spam. Ademas hat 5568 mensajes en un Data Frame escrito en ingles que no son objetos nulos. Por lo tanto, el archivo  se puede leer usando DataFrame en python para clasificar esos mensajes de acuerdo con el indicador dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_msgs():\n",
    "    for index,column in full_corpus.iterrows():\n",
    "        label=column[0]\n",
    "        message_text=column[1]\n",
    "        if label =='ham':\n",
    "            ham_text.append(message_text)\n",
    "        elif label == 'spam':\n",
    "                spam_text.append(message_text)\n",
    "                \n",
    "separate_msgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procesamiento es la tarea de realizar los pasos de preparacion den el corpus de texto sin formato para completar de manera eficiente una extraccion de texto o procesamiento de lenguaje natural o cualquier otra tarea que incluya texto sin formato. El procesamiento de texto consta de varios ppasos, aunque algunos de ellos pueden no aplicarce a una tarea en particular debido a la naturaleza del conjunto de datos disponible.\n",
    "\n",
    "En esta tarea, el preprocesamiento de texto inlcuye los siguientes pasos de acuerdo con el conjunto de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminacion de signos de puntuacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminacion de los signos de puntuacion de los mensajes de correo electronico\n",
    "def remove_msg_punctuations(email_msg):\n",
    "    puntuation_remove_msg=\"\".join([word for word in email_msg if word not in string.punctuation])\n",
    "    return puntuation_removed_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir a minusculas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir a minusculas: La conversion de todos los caracteres del texto en un contexto comun, como los soportes en minusculas impide identificar dos palabaras de manera diferente donde una esta en minuscula y la otra no. Por ejemplo, \"Primero\" y \"primero\" deben identificarse como iguales, por lo tanto poner en minuscula todos los caracteres facilita la tarea.Ademas, las palabras de detencion tambien estan en minusculas, por lo que esto tambien haria posibles eliminar palabras de detencion mas adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing: La tokenizacion es la tarea de dividir el texto en partes significativas, es decir, tokens que incluyen oraciones y palabras. Un token se puede considerar como una instancia de una secuencia de caracteres en un texto particular que se agrupan para proporcionar una unidad semantica util para su posterior procesamiento. En esta tarea, de tokenizacion de palabras se raliza combinado espacios en blanco entre palabras como delimitador.Esto se logra en Python usando expresiones regulares para dividir una cadena en subcadenas con la funcion split(). que es un tokenizador basico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierte el texto en minusculas y tokenizing de palabras\n",
    "def tokenize_into_words(text):\n",
    "    tokens=re.split('\\W+',text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La derivacion es el proceso de eliminar afijos(sufijos, prefijos, infijos, circunfijos) de una palabra para obtener su raiz de palabra. Aunque la lematizacion esta relacionada con la derivacion, difiere ya que la lematizacion puede capturar formas canonicas basadas en el lema de una palabra. La lematizacion ocupa un vocabulario y un analisis morfologico de las palabras que lo hacen mas rapido y preciso que la derivacion. WordNetLemmatizer ha logrado la lematizacion en lenguaje Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "Word_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(tokenized_words):\n",
    "    lemmatized_text = [word_lemmatizer.lemmatize(word)for word in tokenized_words]\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "def preprocessing_msgs(corpus):\n",
    "    categorized_text = pd.DataFrame(corpus)\n",
    "    categorized_text['non_punc_message_body'] = categorized_text[0].apply(lambda msg: remove_msg_punctuations(msg))\n",
    "    categorized_text['tokenized_msg_body'] = categorized_text['non_punc_message_body'].apply(lambda msg: tokenize_into_words(msg.lower()))\n",
    "    categorized_text['lemmatized_msg_words'] = categorized_text['tokenized_msg_body'].apply(lambda word_list: lemmatization(word_list))\n",
    "    return categorized_text['lemmatized_msg_words']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
